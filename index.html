<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>A Tour of GenAI</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/white.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">

    <!-- Custom controls -->
    <link rel="stylesheet" href="plugin/customcontrols/customcontrols.css">
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section>
                <!--
                    Lowercase in headings
                    https://github.com/hakimel/reveal.js/issues/2226
                -->
                <style>
                    .reveal h1,
                    .reveal h2,
                    .reveal h3,
                    .reveal h4,
                    .reveal h5 {
                        text-transform: none;
                    }
                </style>
                <h2>A Tour of GenAI <div style="display: inline" title="To infinity and beyond!">üöÄ</div></h2>
                <p>There and back again...</p>
                <small>
                    <p><a href="https://jgalego.github.io">Jo√£o Galego</a> <div title="Hey there, I'm a brain in a ket!">$$\left|\text{üß†}\right&gt;$$</div></p>
                </small>
            </section>

            <section>
                <section>
                    <h2>Contents üìì</h2>
                    <ol>
                        <li>GenAI in a nutshell üå∞</li>
                        <li>Building GenAI on AWS</li>
                        <li>GenAI in practice</li>
                        <li>References</li>
                    </ol>
                </section>

                <section>
                    <h4>Warning ‚ö†Ô∏è</h4>
                    <p>This presentation is a work in progress‚Ä¶</p>
                    <p class="fragment fade-in">and always will be</p>
                </section>

                <section>
                    <h4>Note on implementation üë®‚Äçüíª</h4>
                    <p>The slides were created using <code><a href="https://revealjs.com/">reveal.js</a></code></p>
                    <p>and the presentation is hosted on <a href="https://danielabaron.me/blog/build-and-publish-presentation-with-html-and-css/">GitHub Pages</a></p>
                </section>

            </section>

            <!-- GenAI in a nutshell -->

            <section>
                <section>
                    <h2>GenAI in a nutshell</h2>
                    <img src="images/walnut.png" />
                </section>

                <section>
                    <h4>In the beginning <br> there was nothing *, <br> which exploded‚Ä¶</h4>
                    <small>
                        <p>‚Äï Terry Pratchett, <a href="https://www.goodreads.com/book/show/34529.Lords_and_Ladies">Lords
                                and Ladies</a> (1992)</p>
                    </small>
                    <br>
                    <!-- source: https://media.tenor.com/tvFWFDXRrmMAAAAd/blow-mind-mind-blown.gif
        <img src="images/mind_blown.gif" width="30%"/-->
                </section>

                <section>
                    <h4>ü•Øüí•</h4>
                    <figure>
                        <img src="images/everything_bagel.gif" title="&quot;I got bored one day, and I put everything on a bagel&quot;" width="70%" />
                    </figure>
                </section>

                <section>
                    <h4>* Well, not exactly‚Ä¶</h4>
                </section>

                <section>
                    <h4>History has a way of repeating itself...</h4>
                    <figure>
                        <img src="images/hieroglyphic_ai.jpg" width="30%" />
                        <figcaption>
                            <small><b>Prompt:</b>
                                <a
                                    href="https://huggingface.co/spaces/stabilityai/stable-diffusion/discussions/new?title=The+history+of+AI+in+hieroglyphs&description=%3Cdiv+style%3D%27display%3A+flex%3B+flex-wrap%3A+wrap%3B+column-gap%3A+0.75rem%3B%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2FHMa340FqXSuttJ3MPiy2s.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2FTz-uNFia4J6HZ6EQtSgID.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2Fd4rRtY-CL6gLP7cwMFQVZ.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2FvfAxjRLa3evSHlEKD6K7q.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2FV3zYgjxqMzw3Jji52ZHxD.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2FZKpvGUYH-DIlb9acjicHN.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2FV8B2qiOnf48mksf3XzCXV.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2Fjkl_d6AhMeh4K7XyopyHe.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3Cimg+src%3D%27https%3A%2F%2Fs3.amazonaws.com%2Fmoonup%2Fproduction%2Fuploads%2Fnoauth%2Fa7gTkIT8bQkH8Ze5JX_wz.jpeg%27+width%3D%27400%27+height%3D%27400%27%3E%0A%3C%2Fdiv%3E">The
                                    history of AI in hieroglyphs</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>1960s: <a href="https://en.wikipedia.org/wiki/ELIZA_effect">The ELIZA Effect</a></h4>
                    <figure>
                        <img src="images/ELIZA_conversation.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://en.wikipedia.org/wiki/ELIZA">Wikipedia</a>
                            </small>
                        </figcaption>
                    </figure>
                    <!--
        
        If you want to learn more about ELIZA, check out Norvig's Paradigms of Artificial Intelligence Programming (PAIP)
        Part II: Early AI Programs > Section 5. ELIZA: Dialog with a Machine
        https://github.com/norvig/paip-lisp/blob/main/docs/chapter5.md
        
        A full LISP implementation is available here:
        https://github.com/norvig/paip-lisp/blob/main/lisp/eliza.lisp
        
        -->
                </section>

                <section>
                    <h4>2022: <a href="https://cognitiontoday.com/the-chatgpt-effect-how-advanced-ai-changes-us/">The
                            ChatGPT Effect</a></h4>
                    <figure>
                        <img src="images/chatgpt_psychologist.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://www.reddit.com/r/ChatGPT/comments/11ecqv9/secret_psychologist/">Reddit</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>
                <section>
                    <h4>Can you spot the differences?</h4>
                    üëÄ
                </section>
                <section>
                    <h4>Let's back up a little‚Ä¶</h4>
                    <!-- source: https://media4.giphy.com/media/v4hXkjXZAuMW5VfReA/200w.gif?cid=6c09b952gm9ybgn3i3bsldz6808lfnxox3vb1u4wib4zy1xe&ep=v1_gifs_search&rid=200w.gif&ct=g -->
                    <img src="images/rewind.gif" />
                </section>

                <section>
                    <h4>2017: Hello Transformers!</h4>
                    <p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> introduces the</p>
                    <p><code>Transformer</code> architecture</p>
                    <figure>
                        <img src="images/transformer.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://bdtechtalks.com/2022/05/02/what-is-the-transformer/">TechTalks</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Motivation</h4>
                    <p>Previous seq2seq models were</p>
                    <p>SLOW üêå and FORGETFUL ü§î</p>
                    <!--
        
        Sometimes in ['catastrophic'](https://en.wikipedia.org/wiki/Catastrophic_interference) ways
        
        -->
                    <figure>
                        <img src="https://static.wixstatic.com/media/3eee0b_969c1d3e8d7943f0bd693d6151199f69~mv2.gif"
                            width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://kikaben.com/transformers-encoder-decoder/">KiKaBeN</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Transformers use <br>an <strong>encoder-decoder</strong> architecture‚Ä¶</h4>
                    <figure>
                        <img src="images/transformer_high_level_architecture.png" width="60%" />
                    </figure>
                </section>

                <section>
                    <h4>‚Ä¶ made of many building blocks üß±</h4>
                    <figure>
                        <img src="https://lena-voita.github.io/resources/lectures/seq2seq/transformer/model-min.png"
                            width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html">NLP Course
                                    | For You</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Zooming in on Attention üîé</h4>
                    <figure>
                        <img src="https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png"
                            width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Lilian Weng</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Visualizing Attention üëÅÔ∏è</h4>
                    <figure>
                        <img src="https://raw.githubusercontent.com/jessevig/bertviz/master/images/head-view.gif"
                            width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/jessevig/bertviz">BertViz</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Deconstructing Attention</h4>
                    <figure>
                        <iframe src="https://player.vimeo.com/video/358488181?h=173570683a" width="640" height="378"
                            frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1">Medium</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Multi-head Attention</h4>
                    <figure>
                        <img src="https://repository-images.githubusercontent.com/162021652/ad9b3712-f067-4c6c-8362-e15b2dd617ed"
                            width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/jessevig/bertviz">BertViz</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>History of Attention</h4>
                    <figure>
                        <img src="images/history_of_attention.jpg" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=XfpMkf4rD6E">Stanford</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><code>Transformers</code> have driven <br>significant progress in AI</h4>
                    <img src="images/progress.gif" width="5%" />
                </section>

                <section>
                    <h4>Decoder-only: <a href="https://openai.com/research/language-unsupervised">GPT-1</a><a
                            href="https://en.wikipedia.org/wiki/GPT-2">.</a><a
                            href="https://en.wikipedia.org/wiki/GPT-3">.</a><a
                            href="https://en.wikipedia.org/wiki/GPT-4">4</a></h4>
                    <figure>
                        <img src="images/gpt_architecture.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://openai.com/research/language-unsupervised">Radford <em>et al.</em>
                                    (2018)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Encoder-only: <a href="https://arxiv.org/abs/1810.04805">BERT</a> and <a
                            href="https://arxiv.org/abs/2002.12327">its
                            progeny</a><!--, [ELECTRA](https://arxiv.org/abs/2003.10555)--></h4>
                    <figure>
                        <img src="images/bert_overall.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/1810.04805">Devlin <em>et al.</em> (2018)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Encoder-decoder: <a href="https://paperswithcode.com/method/t5">T5</a>, <a
                            href="https://huggingface.co/docs/transformers/model_doc/bart">BART</a></h4>
                    <figure>
                        <img src="images/t5_txt2txt_framework.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/1910.10683v3">Raffel <em>et al.</em> (2019)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>How is this all connected with GenAI?</h4>
                    <figure>
                        <!--img src="images/sherlock_mind_palace.gif" width="30%" /-->
                        <img src="images/ff_dyson_frost_wall.png" title="Name the TV show!" width="30%" />
                    </figure>
                </section>

                <section>
                    <h4>Let's focus on the 'generative' part</h4>
                    <figure>
                        <img src="images/focus.gif" width="30%" />
                    </figure>
                </section>

                <section>
                    <h4>There are 2 main classes of statistical models‚Ä¶</h4>
                    <!--figure>
        <img src="images/discriminative_vs_generative.png" width="70%"/>
        <figcaption>
        <small><b>Source:</b> 
        <a href="https://miro.medium.com/v2/resize:fit:1400/1*mi4d8qNEA5owx-EVm9VR1w.png">Medium</a>
        </small>
        </figcaption>
        </figure-->
                </section>

                <section>
                    <h4>Discriminative models</h4>
                    <p>draw boundaries in data space</p>
                    <figure>
                        <img src="images/discriminative.png" width="30%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a
                                    href="https://miro.medium.com/v2/resize:fit:1400/1*mi4d8qNEA5owx-EVm9VR1w.png">Medium</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Example: Van Gogh or not Van Gogh? üëÇüèª</h4>
                    <figure>
                        <img src="images/discriminative_modeling_example.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://vitalflux.com/generative-vs-discriminative-models-examples/">Data
                                    Analytics</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Generative models</h4>
                    <p>describe how data is placed <br> throughout the data space</p>
                    <figure>
                        <img src="images/generative.png" width="30%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a
                                    href="https://miro.medium.com/v2/resize:fit:1400/1*mi4d8qNEA5owx-EVm9VR1w.png">Medium</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Example: Picture me a üê¥</h4>
                    <p>Since the model is probabilistic, <br>we can just <em>sample</em> from it to create new data</p>
                    <figure>
                        <img src="images/generative_modeling_example.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://vitalflux.com/generative-vs-discriminative-models-examples/">Data
                                    Analytics</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <!--
        
        Given inputs $x$ and label $y$
        
        **Discriminative model:**  $p(y | x)$
        
        **Generative model:**   $p(x, y)$ or $p(x)$
        
        -->
                    <h4>We can connect the two using <a href="https://plato.stanford.edu/entries/bayes-theorem/">Bayes'
                            Rule</a></h4>
                    <figure>
                        <img src="images/bayes_theorem.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.kaggle.com/general/220817">Kaggle</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>There are many types of generative models‚Ä¶</h4>
                    <figure>
                        <img src="images/rick_clones.gif" width="40%" />
                    </figure>
                </section>

                <section>
                    <h4>Overview of Generative Models</h4>
                    <figure>
                        <img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png"
                            width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian
                                    Weng</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>2013: Variational Autoencoders (VAE)</h4>
                    <figure>
                        <img src="images/vae.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://learnopencv.com/variational-autoencoder-in-tensorflow/">LearnOpenCV</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>2014: Generative Adversarial Networks (GAN)</h4>
                    <figure>
                        <img src="images/gan.png" width="20%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://d2l.ai">d2l.ai</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>GAN Samples</h4>
                    <figure>
                        <img src="images/gan_samples.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/1406.2661">Goodfellow <em>et al.</em> (2014)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>2015: Diffusion Models</h4>
                    <figure>
                        <img src="images/dm_inpainting.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein <em>et al.</em> (2015)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Diffusion takes a signal and turns it into noise</h4>
                    <p>Signal $\rightarrow$ ‚Ä¶ $\rightarrow$ Noise</p>
                    <img src="images/static_noise.gif" width="10%" />
                </section>

                <section>
                    <h4>Diffusion models are trained to <em>denoise</em> noisy images</h4>
                    <figure>
                        <img src="https://i.imgur.com/dipPOfa.gif" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://keras.io/examples/generative/ddim/">Keras</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <!-- Creatio ex strepitu -->
                    <h4>New images are created by <br> <em>iteratively</em> denoising pure noise</h4>
                    <p>Noise $\rightarrow$ ‚Ä¶ $\rightarrow$ Signal</p>
                    <figure>
                        <img src="https://i.imgur.com/kRXOGzd.gif" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://keras.io/examples/generative/ddim/">Keras</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>January 2021: OpenAI releases DALL-E</h4>
                    <figure>
                        <img src="images/avocado_chairs.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://daleonai.com/dalle-5-mins">Dale on AI</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>DALL-E Samples Comparison</h4>
                    <figure>
                        <img src="images/dalle_samples_comparison.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2102.12092">Ramesh <em>et al.</em> (2021)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>July 2022: Midjourney enters open beta</h4>
                    <figure>
                        <img src="images/midjourney_architecture.gif" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://www.bloomberg.com/news/features/2023-01-31/architects-embrace-ai-art-generator-midjourney">Bloomberg</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>AI-generated paintings as digital art</h4>
                    <p><em>Th√©√¢tre d'√ìpera Spatial</em> (Midjourney + Gigapixel AI)</p>
                    <figure>
                        <img src="images/theatre_dopera_spatial.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html">NYTimes</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><div style="display:inline; background-color: red;">SPOILER ALERT</div> GenAI and Hollywood 2.0</h4>
                    <p>Learn how <a href="https://www.aboutamazon.com/news/aws/how-ai-tools-are-creating-new-possibilities-for-movies-and-visual-design-according-to-this-aws-powered-startup">Runway helped create the rock scene ü™® in <br>'Everything Everywhere All at Once.'</a></p>
                    <figure>
                        <img src="images/eeaao.gif" width="60%" />
                    </figure>
                </section>

                <section>
                    <h4>August 2022: Stability AI releases Stable Diffusion</h4>
                    <figure>
                        <img src="images/stable_van_gogh.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://stability.ai/blog/stable-diffusion-announcement">Stability AI</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Latent Diffusion Model Architecture</h4>
                    <figure>
                        <img src="https://ommer-lab.com/wp-content/uploads/2022/08/article-Figure3-1-1024x508.png"
                            width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://ommer-lab.com/research/latent-diffusion-models/">CompVis Lab</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Stable Diffusion Components</h4>
                    <ol>
                        <li>
                            <p>CLIP (GPT-based) or BERT: Text Encoder</p>
                            <ul>
                                <li><a href="https://arxiv.org/abs/2112.10752">Paper</a>: BERT</li>
                                <li><a href="https://stability.ai/blog/stable-diffusion-public-release">SD v1</a>:
                                    CLIPText</li>
                                <li><a href="https://stability.ai/blog/stable-diffusion-v2-release">SD v2</a>: OpenCLIP
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>UNet + Scheduler: Image Information Creator</p>
                        </li>
                        <li>
                            <p>Autoencoder Decoder: Image Decoder</p>
                        </li>
                    </ol>

                </section>

                <section>
                    <h4>January 2022: InstructGPT</h4>
                    <p>Learning to follow instructions <br> from human preferences</p>
                    <figure>
                        <img src="images/instructgpt_steps.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/1503.03585">Ouyang <em>et al.</em> (2022)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>November 2022: OpenAI releases ChatGPT</h4>
                    <figure>
                        <img src="images/chatgpt_release.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://openai.com/blog/chatgpt">OpenAI</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <img src="images/all_caught_up.gif" width="70%" />
                </section>

                <section>
                    <h3>So, where are we now?</h3>

                </section>

                <section>
                    <h3>GenAI is the fastest growing trend in AI</h3>
                    <p>üìà</p>

                </section>

                <section>
                    <h4>The "Cambrian Explosion" of GenAI</h4>
                    <figure>
                        <img src="images/tree_of_llife.gif" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2304.13712">Yang <em>et al.</em> (2022)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Developer Adoption</h4>
                    <p><a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a> accumulated
                        <code>40k</code> stars <br>on GitHub in its first <code>90</code> days</p>
                    <figure>
                        <img src="images/sd_github_stars.jpg" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://twitter.com/JackSoslow/status/1582384758142672897/photo/1">Twitter</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Consumer Adoption</h4>
                    <p><a href="https://chat.openai.com/chat">ChatGPT</a> reached the <code>1M</code> users mark <br> in
                        just <code>5</code> days</p>
                    <figure>
                        <img src="images/chatgpt_consumer_adoption.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a
                                    href="https://www.linkedin.com/posts/hassaanrahim_ai-chatgpt-internet-activity-7016839518351740928-7izT?utm_source=share&utm_medium=member_desktop">LinkedIn</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>GenAI according to GenAI</h4>
                    <!--
        > "Generative AI refers to a type of AI system that can generate new content, such as images, videos, text, or even music, that is similar to what a human might produce."
        
        <small>
        ‚Äï Adapted from ChatGPT (GPT 3.5)
        </small>
        
        -->
                    <figure>
                        <img src="images/meta_genai.png" width="50%" />
                        <figcaption>
                            <small><b>Prompt:</b> What is Generative AI?
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h3>Let's break it down‚Ä¶</h3>
                    <img src="images/bender_reading.gif" width="30%" />
                </section>

                <section>
                    <p># 1</p>
                    <h4>GenAI can generate new content</h4>
                    <p>similar to what of a human would produce</p>
                </section>

                <section>
                    <h4>Pop Quiz</h4>
                    <p>Which painting was generated with AI?</p>
                    <figure>
                        <img src="images/ai_generated_artwork.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.tidio.com/blog/ai-test/">Tidio</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>If you answered <code>A</code>, you're in big trouble‚Ä¶</h4>
                    <p class="fragment fade-in">or was it the other way around? ü§î</p>
                </section>

                <section>
                    <p># 2</p>
                    <h4>GenAI is powered by <strong>Foundation Models</strong></h4>
                    <p>or FMs for short</p>
                </section>

                <section>
                    <h4>These are really large models‚Ä¶</h4>
                </section>

                <section>
                    <h4>trained on massive amounts of unlabeled data‚Ä¶</h4>
                </section>

                <section>
                    <h4>that can be adapted to a wide range of tasks</h4>
                </section>

                <section>
                    <h4>Traditional vs Foundation Models</h4>
                    <figure>
                        <img src="images/fm_paradigm_shift.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-models-from-amazon-sagemaker-jumpstart-using-the-aws-cdk/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                    <!-- This results in a model that can be adapted to a wide range of downstream tasks -->
                </section>

                <section>
                    <h4>When dealing with natural language, <br>we usually talk about Large Language Models</h4>
                    <p>or LLMs for short</p>
                    <figure>
                        <img src="images/language_modeling.gif" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.amazon.science/blog/responsible-ai-in-the-generative-era">Amazon
                                    Science</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <p>LLMs are really just a <a href="https://hai.stanford.edu/news/reflections-foundation-models">proper
                            subset</a> of FMs</p>
                    <figure>
                        <img src="images/fm_venn.png" width="70%" />
                    </figure>
                </section>

                <section>
                    <h4>How do these models work?</h4>
                    <figure>
                        <img src="images/howto_genai.png" width="90%" />
                    </figure>
                </section>

                <section>
                    <h4>If the input is text-based, we call it a <code>Prompt</code></h4>
                    <figure>
                        <img src="images/prompt_engineering.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://docs.cohere.com/docs/prompt-engineering">cohere</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <p># 3</p>
                    <h4>GenAI applies to <strong>many use cases</strong></h4>
                </section>

                <section>
                    <h4>Some examples include‚Ä¶</h4>
                    <small>
                        <p>Productivity (Text Generation) üí¨</p>
                        <p>Chat (Virtual Assistant) üíÅ</p>
                        <p>Summarization (Text Extraction) üìñ</p>
                        <p>Search üîé</p>
                        <p>Code generation üë®‚Äçüíª</p>
                        <p>Music creation üé∂</p>
                        <p>Video Editing üé•</p>
                    </small>
                </section>

                <section>
                    <h4>GenAI is rapidly transforming AI</h4>
                    <!--img src="https://media.tenor.com/2B3a8aPNwo8AAAAd/borboleta.gif" width="30%" /-->
                    <figure>
                        <video controls muted width="20%">
                            <source src="images/human_stable_evolution.mp4" type="video/mp4">
                        </video>
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.reddit.com/r/Damnthatsinteresting/comments/xzwxex/human_evolution_generated_by_ai_stable_diffusion/">Reddit</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><strong>Text-to-Image</strong> (<code>txt2img</code>)</h4>
                    <figure>
                        <img src="images/txt2img.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://huggingface.co/tasks/text-to-image">ü§ó</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><strong>Image-to-Text</strong> (<code>img2txt</code>)</h4>
                    <figure>
                        <img src="images/img2txt.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://huggingface.co/tasks/image-to-text">ü§ó</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><strong>Image-to-Image</strong> (<code>img2img</code>)</h4>
                    <figure>
                        <img src="images/img2img.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://huggingface.co/tasks/image-to-image">ü§ó</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><strong>Text-to-GIF</strong> (<code>txt2gif</code>)</h4>
                    <figure>
                        <img src="https://user-images.githubusercontent.com/7529846/220882002-72cbfdef-876a-4cb2-9f41-e5989e769868.gif"
                            width="30%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/DN6/giffusion">GIFfusion üí•</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><strong>Text-to-Video</strong> (<code>txt2video</code>)</h4>
                    <figure>
                        <img src="https://makeavideo.studio/assets/overview.webp" width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://makeavideo.studio/">Meta AI</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><strong>Text-to-Code</strong> (<code>txt2code</code>)</h4>
                    <figure>
                        <img src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2023/04/13/codewhisperer_code_generation.v2.gif"
                            width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/aws/amazon-codewhisperer-free-for-individual-use-is-now-generally-available/">AWS
                                    News Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                    <!-- **Anything-to-Anything** (`.*2.*`) -->
                </section>

                <section>
                    <h3>GenAI is taking over the world</h3>
                    <figure>
                        <img src="https://www.sequoiacap.com/wp-content/uploads/sites/6/2022/09/sonya_cute_robots_singing_music_fantasy_cityscape_with_flowers__508554ad-db40-421d-87b6-4a188604f226.png"
                            width="20%" />
                    </figure>
                    <p><strong>"Every industry that requires <br>humans to create original work (‚Ä¶) <br>is up for
                            <strong>reinvention</strong>."</strong></p>
                    <small>
                        ‚Äï Sequoia Capital, <a
                            href="https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/">Generative AI:
                            A Creative New World</a>
                    </small>
                </section>

                <section>
                    <h4>But there are "some" challenges‚Ä¶</h4>
                </section>

                <section>
                    <h4>Hallucinations üçÑ</h4>
                    <figure>
                        <img src="images/fractal_hallucination.jpg" width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/">TheNewStack</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Example: Backwards Epigenetic Inheritance</h4>
                    <p>A <em>causally impossible</em> scientific theory <br>just a prompt away</p>
                    <figure>
                        <img src="images/chatgpt_on_bei.jpg" width="30%" />
                        <figcaption>
                            <small><b>Source:</b> Extracted from
                                <a
                                    href="https://docs.google.com/spreadsheets/d/1kDSERnROv5FgHbVN8z_bXH9gak2IXRtoqz0nwhrviCw">ChatGPT/LLM
                                    Errors</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>"(‚Ä¶) they used to <strong>lie</strong> and say terrible things. <br>Now they just lie and that's
                        interesting enough"</h4>
                    <small>
                        ‚Äï Gary Marcus
                    </small>
                    <!--
        Full quote (https://www.youtube.com/watch?v=Puo3VkPkNZ4): "There are a lot of reasons why AI is starting to come together. I would argue it hasn't fully come together but people got excited about it. Main reason they got excited about it is because we have these chat bots that we've had for a long time, but they used to lie and say terrible things. Now they just lie and that's interesting enough."
        -->
                </section>

                <section>
                    <h4>Actually‚Ä¶</h4>
                </section>

                <section>
                    <p>Philosophically speaking, <br></p>
                    <h4 class="fragment fade-in">LLMs are <a
                            href="https://en.wikipedia.org/wiki/On_Bullshit">üêÇüí©</a>ers not liars</h4>
                    <!--
        Lying vs Bullshiting
        https://reasonandmeaning.com/2020/10/29/lying-vs-bullshit/
        -->
                </section>

                <section>
                    <h4>def. <code>Bullshit</code></h4>
                    <p>Any statement produced without <br> particular concern for reality and truth</p>
                </section>

                <section>
                    <h4>"Bullshit is a greater enemy of truth than lies are."</h4>
                    <small>
                        ‚Äï Harry Frankfurt, <a
                            href="https://press.princeton.edu/books/hardcover/9780691122946/on-bullshit">On Bullshit</a>
                        (2005)
                    </small>
                    <!--
        Extracted from The New Yorker article 'Say Anything' by Jim Holt
        https://www.newyorker.com/magazine/2005/08/22/say-anything
        -->
                </section>

                <section>
                    <h4>Why do these models hallucinate?</h4>
                </section>

                <section>
                    <h4>LLMs as "compact" and "lossy" <br>representations of knowledge</h4>
                    <figure>
                        <img src="images/llm_compact_lossy_knowledge.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination">Designing with Machine Learning</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>How can we prevent/reduce hallucinations?</h4>
                </section>

                <section>
                    <h4>RL with Human Feedback (RLHF)</h4>
                    <p>Human evaluators review the model's responses and pick the most <em>appropriate</em> for the users' prompts</p>
                    <figure>
                        <img src="images/hf_rlhf.png" width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://huggingface.co/blog/rlhf">ü§ó</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Early Detection</h4>
                    <p>Identify hallucinated content and <br>use it during training</p>
                    <figure>
                        <img src="images/early_detection.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2011.02593">Zhou <em>et al.</em> (2021)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Regularization</h4>
                    <p>Often overlooked, these techniques <br>can help alleviate overfitting</p>
                    <figure>
                        <img src="images/dropout_llm.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2305.13230">Xue <em>et al.</em> (2023)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Temperature Tuning üå°Ô∏è</h4>
                    <p>Temperature regulates the randomness <br>or creativity of the responses</p>
                    <small>$\uparrow T \Rightarrow \uparrow \texttt{Hallucinations}$</small>
                    <figure>
                        <img src="images/gpt3_temperature.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination">Designing with Machine Learning</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Chain-of-Thought (CoT) Reasoning ü§î</h4>
                    <p>Using CoT prompting we can improve a model's ability to perform complex reasoning</p>
                    <figure>
                        <img src="images/standard_vs_cot_prompting.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2201.11903">Wei <em>et al.</em> (2022)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>External Data Sources</h4>
                    <p>Provide access to <em>relevant</em> data from a knowledge base</p>
                    <p>Treat the task as a search problem <em>grounded</em> in data</p>
                    <small>
                        <p><a href="https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/">Retrieval Augmented Generation (RAG)</a></p>
                        <p><a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval</a></p>
                        <p><a href="https://arxiv.org/abs/2212.10496">Hypothetical Document Embedding (HyDE)</a></p>
                    </small>
                </section>

                <section>
                    <h4>Security üõ°Ô∏è</h4>
                    <figure>
                        <img src="images/llm_indirect_prompt_injection.png" width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2302.12173">Greshake <em>et al.</em> (2023)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Training LLMs on <em>untrusted</em> data <br> has become the norm rather than the exception</h4>
                    <figure>
                        <img src="images/trustworthy.gif" width="20%" />
                    </figure>
                </section>

                <section>
                    <h4>According to <a href="https://arxiv.org/abs/2305.00944v1">Wan <em>et al.</em> (2023)</a>,
                        launching a successful data poisoning attack during instruction tuning </h4>
                    <p>takes only a few hundred 'poisoned apples' üçé‚ò†Ô∏èü§¢</p>
                    <figure>
                        <img src="images/poisoning_llms.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/alexwan0/poisoning-instruction-tuned-models">GitHub</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Prompt Hacking</h4>
                    <p>Attack that exploits the vulnerabilities of LLMs, <br> by manipulating their inputs or prompts
                    </p>
                    <figure>
                        <img src="images/learn_prompting.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://learnprompting.org/docs/category/-prompt-hacking">Learn Prompting</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Sustainability üå±</h4>
                    <figure>
                        <img src="images/llm_environmental_impact.jpg" width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://pubs.acs.org/doi/10.1021/acs.est.3c01106">Rilling <em>et al.</em>
                                    (2023)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>The open source community has played a key role in this dimension</h4>
                </section>

                <section>
                    <h4>Let me tell you the story of LLaMA ü¶ô</h4>
                </section>

                <section>
                    <h4>February 24th 2023: Meta releases LLaMA ü¶ô</h4>
                    <img src="images/llama.gif" width="30%" />
                </section>

                <section>
                    <h4>March 3rd 2023: LlaMALeaks ü§´</h4>
                    <figure>
                        <img src="images/llama_leaked.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama">Vice</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>March 10th 2023: <code>llama.cpp</code> - initial release</h4>
                    <figure>
                        <img src="images/llamacpp.gif" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://ggml.ai/">GGML</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>March 12th 2023: LlaMA runs on a Raspberry Pi</h4>
                    <figure>
                        <img src="images/llama_rpi.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/ggerganov/llama.cpp/issues/58">GitHub</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>March 13th 2023: Stanford releases Alpaca</h4>
                    <figure>
                        <img src="images/stanford_alpaca.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Training recipe</h4>
                    <figure>
                        <img src="images/alpaca_main.jpg" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Stanford</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>March 14th 2023 ü•ß: LlaMA runs on a Pixel 6</h4>
                    <figure>
                        <img src="images/llama_pixel6.png" width="30%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://twitter.com/thiteanish/status/1635678053853536256">Twitter</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>March 19th 2023: LMSYS Org releases Vicuna</h4>
                    <figure>
                        <img src="images/lmsys_vicuna_comparison.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://lmsys.org/blog/2023-03-30-vicuna/">LMSYS Org</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>May 4th 2023 üååüî´: <a
                            href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither">Moats, moats,
                            moats</a></h4>
                    <figure>
                        <img src="images/moat.jpg" width="50%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.webdonuts.com/2015/07/lazy/">WebDonuts</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <p>üêß</p>
                    <h4>"I often compare open source to science. Science took this whole notion of developing ideas in
                        the open and improving on other people's ideas. It made science what it is today and made the
                        incredible advances that we have had possible."</h4>
                    <small>
                        ‚Äï Linus Torvalds
                    </small>
                </section>

                <section>
                    <h4>What comes next?</h4>
                    <img src="images/stay_tuned.gif" />
                </section>

            </section>

            <!-- Building GenAI on AWS -->

            <section>
                <section>
                    <h2>Building GenAI on AWS</h2>
                    <img src="images/colorful_dc.jpg" width="20%" />
                    <p>For more information, visit <a
                            href="https://aws.amazon.com/generative-ai/">aws.amazon.com/generative-ai</a></p>

                </section>

                <section>
                    <h4>GenAI Workloads</h4>
                    <img src="images/genai_workloads.png" width="80%" />
                </section>

                <section>
                    <h4>The AWS AI/ML Stack (Redux)</h4>
                    <p>AWS supports GenAI in all layers of the stack</p>
                    <img src="images/aws_aiml_stack_redux.png" width="80%" />
                </section>

                <section>
                    <p>Let's start by looking at the bottom layer‚Ä¶</p>
                    <h4>ML Frameworks &amp; Infrastructure</h4>
                </section>

                <section>
                    <h4>There's some evidence that large-scale models <br> lead to better results*</h4>
                    <figure>
                        <img src="images/nn_scaling_laws.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://arxiv.org/abs/2001.08361">Kaplan <em>et al.</em> (2020)</a>
                            </small>
                        </figcaption>
                    </figure>
                    <p class="fragment fade-in"><small>* Read the <a
                                href="https://www.arxiv-vanity.com/papers/2001.08361/#A3">fine print</a>!</small></p>
                </section>

                <section>
                    <h4>AI models are getting bigger‚Ä¶</h4>
                    <figure>
                        <img src="images/drive_to_bigger_models.jpg" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.nature.com/articles/d41586-023-00777-9">Nature</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>‚Ä¶ a lot bigger!</h4>
                    <figure>
                        <img src="images/language_model_sizes.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://lifearchitect.ai/models/">LifeArchitect.ai</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>How do we train a large model like, say‚Ä¶</h4>
                </section>

                <section>
                    <h4>Stable Diffusion?</h4>
                    <img src="images/sd_astronaut.png" />
                </section>

                <section>
                    <h4>Let's check out <a href="https://hpc.stability.ai/">Stability's HPC cluster</a> ü¶Æ</h4>
                    <p>üíª <a
                            href="http://github.com/Stability-AI/stability-hpc">github.com/Stability-AI/stability-hpc</a>
                    </p>
                </section>

                <section>
                    <h4>Training large-scale models comes <br> with a lot of challenges</h4>
                    <small>
                        <p>Hardware üíª</p>
                        <p>Health Checks üë®‚Äç‚öïÔ∏è‚ö†Ô∏è</p>
                        <p>Orchestration üéªüé∂</p>
                        <p>Data üíæ</p>
                        <p>Scale üìà</p>
                        <p>Cost üí∞</p>
                    </small>
                </section>

                <section>
                    <h4>HPC ML cluster for distributed training</h4>
                    <figure>
                        <img src="images/hpc_ml_cluster.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=7I854do63Lg">AWS re:Invent 2022</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://github.com/aws-samples/1click-hpc">1-click HPC</a></h4>
                    <figure>
                        <img src="images/1clickhpc_architecture.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/aws-samples/1click-hpc">GitHub</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Compute: <a href="https://pages.awscloud.com/amazon-ec2-p4d.html">EC2 UltraClusters</a></h4>
                    <figure>
                        <img src="https://d1.awsstatic.com/diagrams/EC2_UltraClusters_HIW.268b99f0bdc37a8696d89db459a2ea71b75e390d.jpg"
                            width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://aws.amazon.com/ec2/instance-types/p4/">AWS</a>
                            </small>
                        </figcaption>
                    </figure>
                    <!--
        
        Over 4000+ accelerators
        
        Fully non-blocking Pb-scale <br> network infrastructure
        
        High-throughput, low-latency storage <br> with FSx for Lustre
        
        -->
                </section>

                <section>
                    <h4>Current NVIDIA A100 GPU Count</h4>
                    <figure>
                        <img src="images/stateofai_a100_count.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.stateof.ai/">State of AI Report 2022</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Compute: <a href="https://aws.amazon.com/ec2/instance-types/trn1/">EC2 <code>Trn1/Trn1n</code>
                            Instances</a></h4>
                    <figure>
                        <img src="https://s4.itho.me/sites/default/files/images/__2021-12-AWS-ML%20instance-Trn1-1.jpg"
                            width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.ithome.com.tw/news/148435">iThome</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Neuron on <code>Trn1</code> Instances</h4>
                    <figure>
                        <img src="https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2022/08/20/trn1-neuron-top.png"
                            width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/aws/amazon-ec2-trn1-instances-for-high-performance-model-training-are-now-available/">AWS
                                    News Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Annapurna Labs</h4>
                    <p>The 'Secret Sauce' behind AWS's success</p>
                    <figure>
                        <img src="https://assets.amazon.science/dims4/default/668f812/2147483647/strip/true/crop/1440x810+0+0/resize/1440x810!/format/webp/quality/90/?url=http%3A%2F%2Famazon-topics-brightspot.s3.amazonaws.com%2Fscience%2F36%2F74%2F6b4c27ed484ca6ef9467ff7c6df7%2Fnafea-bshara-lead-image-alternative072922.png"
                            width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://www.amazon.science/how-silicon-innovation-became-the-secret-sauce-behind-awss-success">Amazon
                                    Science</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Networking: <a href="https://aws.amazon.com/hpc/efa/">Elastic Fabric Adapter (EFA)</a></h4>
                    <figure>
                        <img src="images/ena_vs_efa.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://ieeexplore.ieee.org/document/9167399">Shalev <em>et al.</em> (2020)</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Storage: ML training storage hierarchy</h4>
                    <figure>
                        <img src="images/storage_hierarchy.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=7I854do63Lg">AWS re:Invent 2022</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Orchestration: <a href="https://aws.amazon.com/hpc/parallelcluster/">AWS Parallel Cluster</a>
                    </h4>
                    <figure>
                        <img src="images/hpc_cluster.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=7I854do63Lg">AWS re:Invent 2022</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>How to create a <code>pcluster</code></h4>
                    <small><code><a href="https://github.com/aws/aws-parallelcluster">pcluster</a> create-cluster -f config.yaml ...</code></small>
                    <figure>
                        <img src="images/pc_create_cluster.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=7I854do63Lg">AWS re:Invent 2022</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h3>Is there a better way?</h3>
                    <img src="images/better_way.gif" width="30%"/>
                </section>

                <section>
                    <h4>#1 <a
                            href="https://medium.com/@emilywebber/how-i-trained-10tb-for-stable-diffusion-on-sagemaker-39dcea49ce32">Train
                            Stable Diffusion on Amazon SageMaker</a></h4>
                    <figure>
                        <img src="images/sd_inference_sagemaker.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://medium.com/@emilywebber/how-i-trained-10tb-for-stable-diffusion-on-sagemaker-39dcea49ce32">Medium</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <small>
                        <h3>Step-by-Step Guide</h3>
                        <ol>
                            <li>
                                <p><strong>Prepare Data</strong></p>
                                <ul>
                                    <li>Download <a href="https://laion.ai/blog/laion-5b/"><code>LAION-5B</code></a> Parquet files with <a href="">SageMaker Processing Jobs</a>
                                    </li>
                                    <li>Download <code>LAION-5B</code> images and text pairs</li>
                                    <li>Create <a href="https://aws.amazon.com/fsx/lustre/">FSx for Lustre</a> volume from S3 path</li>
                                    <li>Build <a href="JSON Lines index</li>
                                    <li>Build <a href="https://jsonlines.org/">JSON Lines</a> index</li>
                                </ul>
                            </li>
                            <li>
                                <p><strong>Train Model</strong></p>
                                <ul>
                                    <li>Run on 192 GPUs with <a
                                            href="https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html">SageMaker
                                            distributed training</a></li>
                                </ul>
                            </li>
                            <li>
                                <p><strong>Evaluate Model</strong></p>
                                <ul>
                                    <li>1 epoch on 50M image/text pairs with ~200 GPUs? <strong>15 mins!</strong></li>
                                </ul>
                            </li>
                            <li>
                                <p><strong>Deploy Model</strong></p>
                                <ul>
                                    <li>Run inference using <a
                                            href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">SageMaker
                                            Hosting</a></li>
                                </ul>
                            </li>
                        </ol>
                    </small>
                </section>

                <section>
                    <h4>Accelerate Transformers on AmazonSageMaker <br> with AWS Trainium and AWS Inferentia</h4>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/pokM1r3rgIg"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        allowfullscreen></iframe>
                </section>

                <section>
                    <h4><code>Inf2</code> on Amazon SageMaker</h4>
                    <figure>
                        <img src="images/sagemaker_inf2.jpg" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>#2 <a href="https://aws.amazon.com/sagemaker/jumpstart/getting-started/">Use a pre-trained FM
                            <br> from Amazon SageMaker JumpStart</a></h4>
                    <figure>
                        <img src="images/smj_getting_started.png" width="90%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://aws.amazon.com/sagemaker/jumpstart">AWS</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Models on JumpStart can be accessed in 3 ways</h4>
                    <figure>
                        <img src="images/smj_how_to.png" width="90%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://aws.amazon.com/sagemaker/jumpstart">AWS</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Benefits of pre-trained FMs</h4>
                    <ul>
                        <li>
                            <p>Pre-trained models for each use case</p>
                        </li>
                        <li>
                            <p>Easy to customize + manage models at scale</p>
                        </li>
                        <li>
                            <p>Data is kept secure and private on AWS</p>
                        </li>
                        <li>
                            <p>Responsible AI support across ML lifecycle</p>
                        </li>
                        <li>
                            <p>Fully integrated with Amazon SageMaker</p>
                        </li>
                    </ul>
                </section>

                <section>
                    <h4><a href="https://github.com/jupyterlab/jupyter-ai">Jupyter AI</a>: Bring GenAI to Jupyter
                        Notebooks</h4>
                    <figure>
                        <img src="images/jupyter_ai.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://github.com/jupyterlab/jupyter-ai">GitHub</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>#3 <a
                            href="https://stability.ai/blog/stability-ai-makes-its-stable-diffusion-models-available-on-amazons-new-bedrock-service">Call
                            Amazon Bedrock! ‚õ∞Ô∏è</a></h4>
                    <figure>
                        <img src="images/stability_red_rock.jpg" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://stability.ai/blog/stability-ai-makes-its-stable-diffusion-models-available-on-amazons-new-bedrock-service">Stability.AI</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a></h4>
                    <p>API-level access to FMs</p>
                    <figure>
                        <img src="images/ai_chip_hero.jpg"
                            width="30%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://www.aboutamazon.com/news/aws/aws-amazon-bedrock-generative-ai-service">Amazon</a>
                            </small>
                        </figcaption>
                    </figure>
                    <p>For more information, visit <br><a
                            href="https://aws.amazon.com/bedrock">aws.amazon.com/bedrock</a></p>
                </section>

                <section>
                    <h4>Key Benefits</h4>
                    <figure>
                        <img src="images/bedrock_benefits.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=5EDOTtYmkmI">AWS re:Inforce 2023</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Bedrock supports a wide range of FMs</h4>
                    <figure>
                        <img src="images/bedrock_models.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://aws.amazon.com/bedrock/">AWS</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>You are <u>always</u> in control of your data üéõÔ∏è</h4>
                    <figure>
                        <img src="images/bedrock_security_controls.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a href="https://www.youtube.com/watch?v=5EDOTtYmkmI">AWS re:Inforce 2023</a>
                            </small>
                        </figcaption>
                    </figure>
                    <!--ul>
                        <li><b>Privacy:</b> data is not used to improve base models</li>
                        <li><b>Residency:</b> stays in the Region where it was created</li>
                        <li><b>Isolation:</b> data is isolated per customer</li>
                        <li><b>Encryption:</b> always encrypted in transit and at rest</li>
                    </ul-->
                </section>

                <section>
                    <!--
        
        #### Customize FMs using your organization's data
        
        <figure>
        <img src="images/bedrock_private_fine_tuning.png" width="50%"/>
        <figcaption>
        <small><b>Source:</b> Adapted from
        <a href="https://www.youtube.com/watch?v=5EDOTtYmkmI">AWS re:Inforce 2023</a>
        </small>
        </figcaption>
        </figure>
        
        -->
                    <h4><a href="https://twitter.com/LangChainAI/status/1664301150760497154">Bedrock/LangChain
                            Integration ‚õ∞Ô∏èü¶úüîó</a></h4>
                    <figure>
                        <img src="images/bedrock_langchain_integration.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://github.com/hwchase17/langchain/pull/5464">GitHub</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://aws.amazon.com/codewhisperer/">Amazon CodeWhisperer</a></h4>
                    <p>Build apps faster and more securely <br> with an AI coding companion</p>
                    <figure>
                        <img src="images/codewhisperer_code_generation.v2.gif" width="40%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/aws/amazon-codewhisperer-free-for-individual-use-is-now-generally-available/">AWS
                                    News Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Open-source reference tracking</h4>
                    <figure>
                        <img src="images/codewhisperer_reference_tracking.gif" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/aws/amazon-codewhisperer-free-for-individual-use-is-now-generally-available/">AWS
                                    News Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Security scanning</h4>
                    <figure>
                        <img src="images/codewhisperer_security_scanning.gif" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/aws/amazon-codewhisperer-free-for-individual-use-is-now-generally-available/">AWS
                                    News Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Multiple language and IDE support</h4>
                    <figure>
                        <img src="images/codewhisperer_languages.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://aws.amazon.com/codewhisperer/">AWS</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>"(‚Ä¶) participants who used CodeWhisperer were <strong style="background-color: yellow;">27% more likely</strong> to complete tasks
                            successfully and did so an average of <strong style="background-color: green;">57% faster</strong> than those who
                        didn't use CodeWhisperer."</h4>
                    <small>
                        ‚Äï AWS News Blog
                    </small>
                </section>

                <section>
                    <h4>Build GenAI the easy way with managed services</h4>
                    <figure>
                        <img src="images/genai_managed_services.png" width="80%" />
                    </figure>
                </section>

                <section>
                    <h3>Ready to learn how?</h3>
                    <figure>
                        <img src="images/ready.gif" width="30%" />
                    </figure>
                </section>

            </section>

            <!-- GenAI in Practice -->

            <section>
                <section>
                    <h2>GenAI in Practice</h2>
                    <p>Use Cases, Patterns &amp; Solutions</p>
                    <p>üõ†Ô∏è</p>
                </section>

                <section>
                    <h4>In just a few months, GenAI has exploded‚Ä¶</h4>
                    <figure>
                        <a href="https://theresanaiforthat.com/"><img src="images/taaft.png" width="70%" /></a>
                    </figure>
                </section>

                <section>
                    <h4>GenAI Landscape</h4>
                    <figure>
                        <img src="images/genai_landscape.png" width="90%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://medium.com/data-science-rush/generative-ai-landscape-3d2a4596020f">Medium</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>By the time you read this, <br>the last slide will be completely‚Ä¶</h4>
                    <figure>
                        <img src="images/outdated.gif" width="30%" />
                    </figure>
                </section>

                <section>
                    <h4>Yet, some common patterns are starting to emerge‚Ä¶</h4>
                    <img src="images/lemur.gif" width="40%" />
                </section>

                <section>
                    <h4><a href="https://arxiv.org/abs/2005.11401">Retrieval Augmented Generation (RAG)</a></h4>
                    <figure>
                        <img src="images/smj_rag.jpg" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html">Amazon
                                    SageMaker Developer Guide</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://blog.streamlit.io/langchain-tutorial-3-build-a-text-summarization-app/">Document
                            Summarization</a></h4>
                    <figure>
                        <img src="images/streamlit_text_summarization_app.jpg" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html">Streamlit</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://blog.streamlit.io/langchain-tutorial-4-build-an-ask-the-doc-app/">Document
                            Generation with Facts</a></h4>
                    <figure>
                        <img src="images/streamlit_ask_the_doc.gif" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://blog.streamlit.io/langchain-tutorial-4-build-an-ask-the-doc-app/">Streamlit</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Emerging architectures for LLM applications</h4>
                    <figure>
                        <img src="images/emerging_llm_app_stack.png" width="90%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a
                                    href="https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/">Andreessen Horowitz</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>We can build all of these on AWS</h4>
                    <figure>
                        <img src="images/build.gif" width="30%" />
                    </figure>
                </section>

                <section>
                    <h4><a
                            href="https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/">RAG-based
                            LLM-powered Q&amp;A Bot</a></h4>
                    <figure>
                        <img src="images/qa_bot_opensearch_architecture.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/build-a-powerful-question-answering-bot-with-amazon-sagemaker-amazon-opensearch-service-streamlit-and-langchain/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a
                            href="https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/">RAG
                            workflow with Amazon Kendra and LangChain</a></h4>
                    <figure>
                        <img src="images/rag_workflow_kendra.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Conversational Experience</h4>
                    <figure>
                        <img src="images/rag_workflow_kendra_sample.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a
                            href="https://aws.amazon.com/blogs/machine-learning/introducing-an-image-to-speech-generative-ai-application-using-amazon-sagemaker-and-hugging-face/">Image-to-Speech
                            app using <br> Amazon SageMaker and ü§ó</a></h4>
                    <figure>
                        <img src="images/describe4me_architecture.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/introducing-an-image-to-speech-generative-ai-application-using-amazon-sagemaker-and-hugging-face/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a
                            href="https://aws.amazon.com/es/blogs/machine-learning/virtual-fashion-styling-with-generative-ai-using-amazon-sagemaker/">Virtual
                            fashion styling <br> using Amazon SageMaker üëí</a></h4>
                    <figure>
                        <img src="images/virtual_tryon.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/machine-learning/virtual-fashion-styling-with-generative-ai-using-amazon-sagemaker/">AWS
                                    ML Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://www.pinecone.io/learn/vector-database/">Vector Databases</a></h4>
                    <figure>
                        <img src="images/vector_database.png" width="70%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a href="https://www.pinecone.io/learn/vector-database/">Pinecone</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>Vector databases are useful for storing embeddings</h4>
                    <p>since they treat vectors as first class citizens</p>
                </section>

                <section>
                    <h4>A Quick Primer on Embeddings</h4>
                    <p>A numerical representation of a piece of information</p>
                    <figure>
                        <img src="images/embeddings_primer.png" width="80%" />
                        <figcaption>
                            <small><b>Source:</b> Adapted from
                                <a
                                    href="https://arize.com/blog-course/embeddings-meaning-examples-and-how-to-compute/">Arize</a>
                                and <a
                                    href="https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8">Medium</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h3>How can we have a <br> <a
                            href="https://aws.amazon.com/what-is/vector-databases/">vector database on AWS</a>?</h3>
                </section>

                <section>
                    <h4>#1 <a
                            href="https://aws.amazon.com/blogs/database/building-ai-powered-search-in-postgresql-using-amazon-sagemaker-and-pgvector/">RDS
                            for PgSQL + <code>pgvector</code> extension</a></h4>
                    <pre><code class="language-sql">CREATE TABLE test_embeddings(product_id bigint, embeddings vector(3) );
        
        INSERT INTO test_embeddings VALUES
        (1, '[1, 2, 3]'), (2, '[2, 3, 4]'), (3, '[7, 6, 8]'), (4, '[8, 6, 9]');
        
        SELECT product_id, embeddings, embeddings &lt;-&gt; '[3,1,2]' AS distance
        FROM test_embeddings 
        ORDER BY embeddings &lt;-&gt; '[3,1,2]';
        
        /*
         product_id | embeddings |     distance
        ------------+------------+-------------------
                  1 | [1,2,3]    | 2.449489742783178
                  2 | [2,3,4]    |                 3
                  3 | [7,6,8]    | 8.774964387392123
                  4 | [8,6,9]    |   9.9498743710662
        */
        </code></pre>
                </section>

                <section>
                    <h4><a
                            href="https://aws.amazon.com/blogs/database/building-ai-powered-search-in-postgresql-using-amazon-sagemaker-and-pgvector/">Using
                            a similarity search for enhancing <br> product catalog search in an online retail store</a>
                    </h4>
                    <figure>
                        <img src="images/rds_pgvector_architecture.png" width="60%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/database/building-ai-powered-search-in-postgresql-using-amazon-sagemaker-and-pgvector/">AWS
                                    Database Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>#2 <a href="https://opensearch.org/platform/search/vector-database.html">OpenSearch</a></h4>
                    <figure>
                        <img src="images/amazon_music_embeddings.png" width="50%" />
                        <figcaption>
                            <small><b>Source:</b>
                                <a
                                    href="https://aws.amazon.com/blogs/big-data/amazon-opensearch-services-vector-database-capabilities-explained/">AWS
                                    Big Data Blog</a>
                            </small>
                        </figcaption>
                    </figure>
                </section>

                <section>
                    <h4>#3 <a
                            href="https://medium.com/swlh/add-similarity-search-to-dynamodb-with-faiss-c68eb6a48b08">DynamoDB
                            + FAISS</a></h4>
                    <pre><code class="language-python">def save_faiss_model(self, text_list, id_list):
            # Convert abstracts to vectors
            embeddings = model.encode(text_list, show_progress_bar=False)
            # Step 1: Change data type
            embeddings32 = np.array(
                [embedding for embedding in embeddings]).astype(&quot;float32&quot;)
            # Step 4: Add vectors and their IDs
            index_start_id = self.index.ntotal # inclusive
            self.index.add(embeddings32)
            index_end_id = self.index.ntotal # exclsuive
            # serialize index
            Path(f&quot;{FAISS_DIR}/{self.content_group}&quot;).mkdir(parents=True, exist_ok=True)
            faiss.write_index(self.index, f&quot;{FAISS_DIR}/{self.content_group}/{self.content_group}_faiss_index.bin&quot;)
            return (embeddings32, range(index_start_id, index_end_id))
        </code></pre>
                </section>

                <section>
                    <h4>#4 <a
                            href="https://medium.com/swlh/add-similarity-search-to-dynamodb-with-faiss-c68eb6a48b08">AWS Marketplace</a> solution</h4>
                    <figure>
                        <img src="images/aws_marketplace_vector_databases.png" width="70%"/>
                    </figure>
                </section>

                <section>
                    <h3>What if I want to explore <br> more use cases?</h3>
                    <figure>
                        <img src="images/explore.gif" width="30%" />
                    </figure>
                </section>

                <section>
                    <h4><a href="https://aiexplorer.aws.amazon.com/">AI Use Case Explorer</a></h4>
                    <p>Find the most relevant AI use cases with <br> related content and guidance to make them real</p>
                    <!-- Adapted from https://aws.amazon.com/blogs/publicsector/3-ways-tax-agencies-can-use-ai-aws/ -->
                    <img src="images/ai_use_case_explorer.png" width="70%" />
                </section>

                <section>
                    <h4><code>Hello World</code>: Meet Generative AI</h4>
                    <p>Werner Vogels and Swami Sivasubramanian <br>sit down to discuss GenAI and why it's not a hype</p>
                    <figure>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/dBzCGcwYCJo"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        allowfullscreen style="margin-top: 10;"></iframe>
                    </figure>
                </section>

                <section>
                    <h4><a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">Hands-On GenAI with LLMs
                            Course</a></h4>
                    <p>Learn the fundamentals of how GenAI works and <br> how to deploy it in real-world applications</p>
                    <img src="images/genai_with_llms_course.png" width="70%" />
                </section>

                <section>
                    <h4><a href="https://aws-startup-lofts.com/amer/program/accelerators/generative-ai">AWS Generative
                            AI Accelerator</a></h4>
                    <p>Accelerate your GenAI startup in 10 weeks</p>
                    <img src="images/aws_genai_accelerator.png" width="70%" />
                </section>
            </section>

            <section>
                <section>
                    <h2>References üìö</h2>
                    <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90cb787d-c3a1-48c4-86d1-84e7456a949a_500x213.gif"
                        width="50%" />
                </section>

                <section>
                    <h4>Transformers üöóü§ñ‚öîÔ∏è</h4>
                    <small>
                        <ul>
                            <li><a href="https://arxiv.org/abs/1706.03762">The original paper</a> by Vaswani <em>et
                                    al.</em></li>
                            <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/">The annotated version</a>
                                by Harvard NLP</li>
                            <li><a href="https://jalammar.github.io/illustrated-transformer/">The illustrated
                                    version</a> by Jay Alammar</li>
                            <li><a href="https://twitter.com/abhi1thakur/status/1470406419786698761">The</a> <a
                                    href="https://twitter.com/MishaLaskin/status/1479246928454037508">Twitter</a> <a
                                    href="https://twitter.com/0xsanny/status/1176517584319127553?lang=en">version</a> by
                                <code>abhi1thakur</code>, <code>MishaLaskin</code> and <code>0xsanny</code></li>
                        </ul>
                    </small>
                </section>

                <section>
                    <h4>Diffusers üß®</h4>
                    <small>
                        <ul>
                            <li><a href="https://arxiv.org/abs/2006.11239">The original paper</a> by Ho <em>et al.</em>
                            </li>
                            <li><a href="https://huggingface.co/blog/annotated-diffusion">The annotated version</a> by
                                ü§ó</li>
                            <li><a href="https://jalammar.github.io/illustrated-stable-diffusion/">The illustrated
                                    version</a> by Jay Alammar</li>
                            <li><a href="https://twitter.com/iScienceLuvr/status/1592860019057250304">The Twitter
                                    version</a> by <code>iScienceLuvr</code></li>
                        </ul>
                    </small>
                </section>

                <section>
                    <h4>Courses üë©‚Äçüè´</h4>
                    <small>
                        <ul>
                            <li><a href="https://www.cs.utexas.edu/users/flame/laff/alaff/ALAFF.html">ALAFF</a>:
                                Advanced Linear Algebra - Foundations to Frontiers</li>
                            <li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX">Statistics
                                    110</a>: Probability</li>
                            <li><a
                                    href="https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX">CS221</a>:
                                Artificial Intelligence - Principles and Techniques</li>
                            <li><a
                                    href="https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM">CS25</a>:
                                Transformers United</li>
                            <li><a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">COS597G</a>:
                                Understanding Large Language Models</li>
                            <li><a href="https://web.stanford.edu/class/cs224n/">CS224N</a>: Natural Language Processing
                                with Deep Learning</li>
                            <li><a
                                    href="https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ">CS224U</a>:
                                Natural Language Understanding</li>
                            <li><a href="https://stanford-cs324.github.io/winter2022/">CS324</a>: Large Language Models
                            </li>
                            <li><a
                                    href="https://youtube.com/playlist?list=PLWnsVgP6CzaelCF_jmn5HrpOXzRAPNjWj">CS685</a>:
                                Advanced Natural Language Processing</li>
                            <li><a href="https://rycolab.io/classes/llm-s23/">263-5354-00L</a>: Large Language Models</li>
                        </ul>
                    </small>
                </section>

                <section>
                    <h4>Courses üëæ</h4>
                    <small>
                        <ul>
                            <li><a href="https://karpathy.ai/zero-to-hero.html">Neural Networks: Zero to Hero</a>
                                by Andrej Karpathy</li>
                            <li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">NLP Course</a>
                                by ü§ó</li>
                            <li><a href="https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/">LangChain for LLM Application Development</a>
                                by DeepLearning.ai</li>
                            <li><a href="https://github.com/mlabonne/llm-course">Large Language Model Course</a>
                                by <code>mlabonne</code></li>
                            <li><a href="https://learnprompting.org/docs/intro">Learn Prompting</a></li>
                        </ul>
                    </small>
                </section>

                <section>
                    <h4>Meta ‚ôæÔ∏è</h4>
                    <small>
                        <p><strong>Disclaimer:</strong> I take no responsibility for the content available through these links</p>
                        <ul>
                            <li><a href="https://sebastianraschka.com/blog/2023/llm-reading-list.html">Understanding LLMs - A Transformative Reading List</a> 
                                by Sebastian Raschka</li>
                            <li><a href="https://github.com/Mooler0410/LLMsPracticalGuide">LLMs Practical Guide</a>
                                by <code>Mooler0410</code></li>
                            <li><a href="https://github.com/Hannibal046/Awesome-LLM">Awesome LLM</a> 
                                by <code>Hannibal046</code></li>
                            <li><a href="https://github.com/kyrolabs/awesome-langchain">Awesome LangChain</a>
                                by Kyrolabs</li>
                            <li><a href="https://aman.ai/">aman.ai</a>
                                by Aman Chada</li>
                        </ul>
                    </small>
                </section>
            </section>
        </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/search/search.js"></script>
    <script src="plugin/math/math.js"></script> <!-- https://revealjs.com/math/ -->
    <script src="plugin/menu/menu.js"></script> <!-- https://github.com/denehyg/reveal.js-menu -->
    <script src="plugin/customcontrols/customcontrols.js"></script>
    <script>
        // More info about initialization & config:
        // - https://revealjs.com/initialization/
        // - https://revealjs.com/config/
        Reveal.initialize({
            hash: true,

            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [
                RevealMarkdown,
                RevealHighlight,
                RevealNotes,
                RevealMath.KaTeX,
                RevealMenu,
                RevealCustomControls,
                RevealSearch
            ],

            customcontrols: {
                controls: [
                    {
                        id: 'toggle-overview',
                        title: 'Toggle overview (O)',
                        icon: '<i class="fa fa-th"></i>',
                        action: 'Reveal.toggleOverview();'
                    }
                ]
            },

            menu: {
                numbers: 'c',
                openSlideNumber: true,
                themes: true,
                themesPath: 'lib/reveal.js/dist/theme/',
                transitions: true,
                custom: [
                    {
                        title: 'Blogs',
                        icon: '<i class="fa fa-bookmark">',
                        src: 'links.html'
                    }
                ]
            }
        });
    </script>
</body>

</html>